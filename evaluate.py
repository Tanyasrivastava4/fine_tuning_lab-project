# evaluate.py
import json
import torch
from datasets import load_from_disk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel
from sklearn.metrics import accuracy_score, confusion_matrix
from torch.utils.data import DataLoader
import wandb

# ------------------------------
# 1Ô∏è‚É£ Initialize W&B
# ------------------------------
wandb.init(project="phi3_finetune_support_tickets", name="evaluation")

# ------------------------------
# 2Ô∏è‚É£ Load tokenizer from fine-tuned adapter
# ------------------------------
tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_model")

# ------------------------------
# 3Ô∏è‚É£ Load base model with correct number of labels
# ------------------------------
num_labels = 100  # same as training
base_model_name = "microsoft/phi-3-mini-4k-instruct"
base_model = AutoModelForSequenceClassification.from_pretrained(
    base_model_name,
    num_labels=num_labels
)

# ------------------------------
# 4Ô∏è‚É£ Load PEFT LoRA adapter safely
# ------------------------------
model = PeftModel.from_pretrained(base_model, "./fine_tuned_model")
model.eval()

# ------------------------------
# 5Ô∏è‚É£ Move model to GPU if available
# ------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# ------------------------------
# 6Ô∏è‚É£ Load test dataset
# ------------------------------
test_dataset = load_from_disk("prepared_data/prepared_data/test")

# ------------------------------
# 7Ô∏è‚É£ Tokenize test dataset
# ------------------------------
def tokenize(batch):
    return tokenizer(batch['text'], padding=True, truncation=True)

test_dataset = test_dataset.map(tokenize, batched=True)

# ------------------------------
# 8Ô∏è‚É£ Set format for PyTorch
# ------------------------------
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# ------------------------------
# 9Ô∏è‚É£ Create DataLoader for batched inference
# ------------------------------
batch_size = 32  # adjust based on GPU memory
dataloader = DataLoader(test_dataset, batch_size=batch_size)

preds = []
labels = []

# ------------------------------
# üîü Batched predictions
# ------------------------------
for batch in dataloader:
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        batch_preds = torch.argmax(outputs.logits, dim=-1)
        preds.extend(batch_preds.cpu().tolist())
        labels.extend(batch['label'].tolist())

# ------------------------------
# 1Ô∏è‚É£1Ô∏è‚É£ Compute metrics
# ------------------------------
acc = accuracy_score(labels, preds)
cm = confusion_matrix(labels, preds).tolist()

metrics = {
    "accuracy": acc,
    "confusion_matrix": cm
}

# ------------------------------
# 1Ô∏è‚É£2Ô∏è‚É£ Save metrics to JSON
# ------------------------------
with open("metrics.json", "w") as f:
    json.dump(metrics, f, indent=4)

# ------------------------------
# 1Ô∏è‚É£3Ô∏è‚É£ Log metrics to W&B
# ------------------------------
wandb.log({"eval_accuracy": acc})

# Optional: log confusion matrix for first 200 predictions
#import pandas as pd
#sample_size = 200
#sample_cm = confusion_matrix(labels[:sample_size], preds[:sample_size])
#wandb.log({"confusion_matrix_sample": wandb.Table(data=sample_cm)})

import matplotlib.pyplot as plt
import seaborn as sns

sample_size = 200
sample_cm = confusion_matrix(labels[:sample_size], preds[:sample_size])

plt.figure(figsize=(6, 5))
sns.heatmap(sample_cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix Sample")
plt.tight_layout()

wandb.log({"confusion_matrix_sample": wandb.Image(plt)})
plt.close()

wandb.finish()

print(f"‚úÖ Evaluation complete. Accuracy: {acc:.4f}")
print("üìä Metrics saved to metrics.json and logged to Weights & Biases")


# Use this code when you are not able to see the output in wandb clearly.
# evaluate_wandb.py
#import json
#import torch
#from datasets import load_from_disk
#from transformers import AutoTokenizer, AutoModelForSequenceClassification
#from peft import PeftModel
#from sklearn.metrics import accuracy_score, confusion_matrix
#from torch.utils.data import DataLoader
#import wandb
#import matplotlib.pyplot as plt
#import seaborn as sns

# 1Ô∏è‚É£ Initialize W&B project
#wandb.init(project="phi3_finetune_support_tickets", name="evaluation_full")

# 2Ô∏è‚É£ Load tokenizer and base model
#tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_model")
#num_labels = 100  # same as training
#base_model_name = "microsoft/phi-3-mini-4k-instruct"
#base_model = AutoModelForSequenceClassification.from_pretrained(
 #   base_model_name,
  #  num_labels=num_labels
#)

# 3Ô∏è‚É£ Load PEFT LoRA adapter safely
#model = PeftModel.from_pretrained(base_model, "./fine_tuned_model")
#model.eval()

# 4Ô∏è‚É£ Move model to GPU if available
#device = "cuda" if torch.cuda.is_available() else "cpu"
#model.to(device)

# 5Ô∏è‚É£ Load test dataset
#test_dataset = load_from_disk("prepared_data/prepared_data/test")

# 6Ô∏è‚É£ Tokenize test dataset
#def tokenize(batch):
 #   return tokenizer(batch['text'], padding=True, truncation=True)

#test_dataset = test_dataset.map(tokenize, batched=True)

# 7Ô∏è‚É£ Set format for PyTorch
#test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# 8Ô∏è‚É£ Create DataLoader
#batch_size = 32
#dataloader = DataLoader(test_dataset, batch_size=batch_size)

#preds = []
#labels = []

# 9Ô∏è‚É£ Batched predictions
#for batch in dataloader:
 #   input_ids = batch['input_ids'].to(device)
  #  attention_mask = batch['attention_mask'].to(device)
   # with torch.no_grad():
    #    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
     #   batch_preds = torch.argmax(outputs.logits, dim=-1)
      #  preds.extend(batch_preds.cpu().tolist())
       # labels.extend(batch['label'].tolist())

# üîü Compute metrics
#acc = accuracy_score(labels, preds)
#cm = confusion_matrix(labels, preds).tolist()

#metrics = {
 #   "accuracy": acc,
  #  "confusion_matrix": cm
#}

# 1Ô∏è‚É£1Ô∏è‚É£ Save metrics to JSON
#with open("metrics.json", "w") as f:
 #   json.dump(metrics, f, indent=4)

# 1Ô∏è‚É£2Ô∏è‚É£ Log metrics to W&B
#wandb.log({"eval_accuracy": acc})

# 1Ô∏è‚É£3Ô∏è‚É£ Log confusion matrix as an image
#plt.figure(figsize=(8, 6))
#sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
#plt.xlabel("Predicted")
#plt.ylabel("Actual")
#plt.title("Confusion Matrix")
#plt.tight_layout()
#wandb.log({"confusion_matrix": wandb.Image(plt)})
#plt.close()

# 1Ô∏è‚É£4Ô∏è‚É£ Log sample predictions (first 50 examples)
#sample_size = 50
#data = list(zip(labels[:sample_size], preds[:sample_size]))
#table = wandb.Table(data=data, columns=["Actual Label", "Predicted Label"])
#wandb.log({"sample_predictions": table})

#wandb.finish()
#print(f"‚úÖ Evaluation complete. Accuracy: {acc:.4f}")
#print("üìä Metrics, confusion matrix, and sample predictions logged to W&B")
